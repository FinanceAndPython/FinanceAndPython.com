{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Data\n",
    "\n",
    "Pandas has many options for writing data including to csv which is usually the most popular, to excel, to json, etc. The most basic usage is to call to_csv on a dataframe with the argument of the filepath/filename. If you do just specify a file name then you will end up writing the file to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Create test data\n",
    "test_data = pd.DataFrame([[1, 2, 3],\n",
    "                         [4, 5, 6],\n",
    "                         [7, 8, 9]], index=['A','B','C'], columns=[\"Col 1\", \"Col 2\", \"Col 3\"])\n",
    "\n",
    "#Write the test data to csv\n",
    "test_data.to_csv(\"TestData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data\n",
    "\n",
    "Within pandas there is read_csv and similar files which will read in data. We can call this with a path to a file and it will return back a dataframe. For example, let's read in the data we just wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0  Col 1  Col 2  Col 3\n",
      "0          A      1      2      3\n",
      "1          B      4      5      6\n",
      "2          C      7      8      9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"TestData.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing this you can specify the index column before reading in. Above you see that if we do not do that then pandas assumes there is no index and all columns should be in the dataset. Instead let's read in with the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Col 1  Col 2  Col 3\n",
      "A      1      2      3\n",
      "B      4      5      6\n",
      "C      7      8      9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"TestData.csv\", index_col=0)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toggling Index and Header\n",
    "\n",
    "When we are using functions to write data, in all the cases we can also use optional arguments such as index and header to toggle writing the columns and the index. Let's make a second file without index or header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"TestData2.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read in the data that we just wrote and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  2  3\n",
      "0  4  5  6\n",
      "1  7  8  9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"TestData2.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is going to always assume that the first row is the header of your data. To read in the data assuming no header you can pass the argument header=None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2\n",
      "0  1  2  3\n",
      "1  4  5  6\n",
      "2  7  8  9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"TestData2.csv\", header=None)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using os to Modify Files\n",
    "\n",
    "The os library will allow you to make certain changes to your file system or modify files which can make your life much easier when you start doing things like writing hundreds of files down. Let's begin with a simple example. If you want to make a folder in your current directory, you can call os.mkdir with the folder name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Create a folder called Test\n",
    "\n",
    "os.mkdir(\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind if you try to create the directory a second time it will throw an error because it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'Test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0d0116e179a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'Test'"
     ]
    }
   ],
   "source": [
    "os.mkdir(\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing File Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are also able to list a directory's contents to better understand where you are. The os.listdir function takes a path and returns the contents in that path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '1 Pandas Basics.ipynb', '2 Data Transformations.ipynb', '3 Statistics.ipynb', '4 Reading and Writing Data.ipynb', '5 Joins.ipynb', '6 Grouping.ipynb', '7 Introduction to numpy.ipynb', '8 Randomness.ipynb', 'Test', 'TestData.csv', 'TestData2.csv']\n"
     ]
    }
   ],
   "source": [
    "#List the contents of the current directory\n",
    "print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#List the contents of the Test directory\n",
    "print(os.listdir(\"./Test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing and Renaming Files\n",
    "\n",
    "Remove can be done with os.remove for files or os.rmdir for directories. The function os.rename takes first a path to a current file and then the new name and renames it to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the first test data\n",
    "os.remove(\"TestData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the second test data set\n",
    "os.rename(\"TestData2.csv\", \"TestData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '1 Pandas Basics.ipynb', '2 Data Transformations.ipynb', '3 Statistics.ipynb', '4 Reading and Writing Data.ipynb', '5 Joins.ipynb', '6 Grouping.ipynb', '7 Introduction to numpy.ipynb', '8 Randomness.ipynb', 'Test', 'TestData.csv']\n"
     ]
    }
   ],
   "source": [
    "#List the directory contents\n",
    "print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove both the new file and the new directory\n",
    "os.remove(\"TestData.csv\")\n",
    "os.rmdir(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '1 Pandas Basics.ipynb', '2 Data Transformations.ipynb', '3 Statistics.ipynb', '4 Reading and Writing Data.ipynb', '5 Joins.ipynb', '6 Grouping.ipynb', '7 Introduction to numpy.ipynb', '8 Randomness.ipynb']\n"
     ]
    }
   ],
   "source": [
    "#List the directory\n",
    "print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Chunks\n",
    "\n",
    "A final feature that can be a lifesaver is the ability to read in chunks. What this does is allows you to take a large dataset and only read in pieces at a time to make it more manageable. Let's begin with creating an example file with 100 rows (in reality we would only do this with a large dataset, but this is just to show the example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create some dummy data\n",
    "test_data = pd.DataFrame([[x, x**2, x*5] for x in range(1, 101)])\n",
    "test_data.to_csv(\"TestData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple objective is to get the sum of all values in the dataframe. In our example, you can image that instead of 100 values we have 100 million+ values and so we may not be able to read the dataset into our computer depending on how much memory the computer has. An important thing to ask yourself is whether or not what you are trying to achieve can be done in chunks.... there are times that you can only run operations on the full dataset for one reason or another and if that is the case your best bet is moving to a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you give the chunksize argument with a number of rows to read in each time, you will be able to piece the reading into parts. The code below will chunk the dataframe into sets of 10 rows and then we can loop through each chunk to get the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of current chunk: 715\n",
      "\n",
      "Sum of current chunk: 3415\n",
      "\n",
      "Sum of current chunk: 8115\n",
      "\n",
      "Sum of current chunk: 14815\n",
      "\n",
      "Sum of current chunk: 23515\n",
      "\n",
      "Sum of current chunk: 34215\n",
      "\n",
      "Sum of current chunk: 46915\n",
      "\n",
      "Sum of current chunk: 61615\n",
      "\n",
      "Sum of current chunk: 78315\n",
      "\n",
      "Sum of current chunk: 97015\n",
      "\n",
      "Total sum: 368650\n"
     ]
    }
   ],
   "source": [
    "chunks = pd.read_csv(\"TestData.csv\", index_col=0,chunksize=10)\n",
    "total = 0\n",
    "for chunk in chunks:\n",
    "    s = chunk.sum().sum()\n",
    "    print(\"Sum of current chunk: {}\".format(s))\n",
    "    print()\n",
    "    total += s\n",
    "print(\"Total sum: {}\".format(total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
